#그런데, 이 값은 매우 불안정하다. seed = 227만 봐도 알 수 있듯이... 따라서 다른 모델을 시도해 본다.
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)
lm1_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x7, data = trn1)
MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)
lm2_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9, data = trn1)
MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)
lm3_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)
MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam_df1)
summary(gam_df1)
MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam2
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)
lm1_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x7, data = trn1)
MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)
lm2_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9, data = trn1)
MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)
lm3_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)
MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam_df1)
summary(gam_df1)
MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam2
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df1 <- glmnet(X_df1, y_df1, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean(predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2
MSE_df1_lasso
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)
lm1_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x7, data = trn1)
MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)
lm2_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9, data = trn1)
MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)
lm3_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)
MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam_df1)
summary(gam_df1)
MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_gam2
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df1 <- glmnet(X_df1, y_df1, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean(predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2
MSE_df1_lasso
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
X_df1 <-
model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
X_df1 <-
model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
ridge_df1 <- glmnet(X_df1, y_df1, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean(predict.glmnet(ridge_df1, Xtst_df1) - tst1$y)^2
MSE_df1_ridge
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df1 <- glmnet(X_df1, y_df1, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean(predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2
MSE_df1_lasso
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
ridge_df1 <- glmnet(X_df1, y_df1, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean(predict.glmnet(ridge_df1, Xtst_df1) - tst1$y)^2
MSE_df1_ridge
MSE_df1_lm1
MSE_df1_lm2
MSE_df1_lm3
MSE_df1_ridge
MSE_df1_lasso
MSE_df1_gam1
MSE_df1_gam2
predict(gam2_df1, newx = df1_test)
?predict.Gam
predict(gam2_df1, newdata = df1_test)
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam_df1)
summary(gam_df1)
MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam2
MSE_df1_lm1
MSE_df1_lm2
MSE_df1_lm3
MSE_df1_ridge
MSE_df1_lasso
MSE_df1_gam1
MSE_df1_gam2
final_csv[,2] <- predict(gam2_df1, newdata = df1_test)
View(final_csv)
View(final_csv)
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
final_csv <- read.csv("./dataset/test_prediction.csv")
colnames_final <- colnames(final_csv)
df1_train <- read.csv("./dataset/dataset1_train.csv")
df1_test <- read.csv("./dataset/dataset1_test.csv")
df1_train
summary(df1_train$y)
set.seed(42)
data_split_df1 <- initial_split(df1_train, prop = 0.9)
trn1 <- training(data_split_df1)
tst1 <- testing(data_split_df1)
best_select_df1 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10)
sum_lm1 <- summary(best_select_df1)
which.min(sum_lm1$cp)
which.min(sum_lm1$bic)
which.min(sum_lm1$adjr2)
pairs(~x2 + x5 + x7 + x9 + x10 + y, data = trn1)
pairs(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + y, data = trn1)
ggcorrplot(cor(trn1), lab = T, lab_size = 2)
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)
lm1_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x7, data = trn1)
MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)
lm2_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9, data = trn1)
MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)
lm3_df1 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)
MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam1_df1)
summary(gam2_df1)
MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam2
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df1 <- glmnet(X_df1, y_df1, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean(predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2
MSE_df1_lasso
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
ridge_df1 <- glmnet(X_df1, y_df1, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean(predict(ridge_df1, Xtst_df1) - tst1$y)^2
MSE_df1_ridge
MSE_df1_lm1
MSE_df1_lm2
MSE_df1_lm3
MSE_df1_gam1
MSE_df1_gam2
MSE_df1_ridge
MSE_df1_lasso
final_csv[,2] <- predict(gam2_df1, newdata = df1_test)
df2_train <- read.csv("./dataset/dataset2_train.csv")
df2_test <- read.csv("./dataset/dataset2_test.csv")
df2_train
summary(df2_train)
sum(df2_train$y)/800
ggcorrplot(cor(df2_train), lab = T, lab_size = 2)
set.seed(42)
data_split_df2 <- initial_split(df2_train, prop = 0.9)
trn2 <- training(data_split_df2)
tst2 <- testing(data_split_df2)
X_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)[,-1]
y_df2 <- trn2$y
Xtst_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst2)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df2
pred <- ifelse(predict(lasso_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_lasso <- mean(pred - tst2$y)^2
MSE_df2_lasso
set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
ridge_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df2
pred <- ifelse(predict(ridge_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_ridge <- mean(pred - tst2$y)^2
MSE_df2_ridge
df3_train <- read.csv("./dataset/dataset3_train.csv")
df3_test <- read.csv("./dataset/dataset3_test.csv")
df3_train
summary(df3_train)
pairs(~., data = df3_train)
hist(df3_train$y)
set.seed(42)
data_split_df3 <- initial_split(df3_train, prop = 0.9)
trn3 <- training(data_split_df3)
tst3 <- testing(data_split_df3)
df3_train %>% filter(y > 10)
trn3_drop <- trn3 %>% filter(y < 7.4)
best_select_df3 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10)
sum_lm3 <- summary(best_select_df3)
which.min(sum_lm3$cp)
which.min(sum_lm3$bic)
which.min(sum_lm3$adjr2)
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 1)
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 2)
ggcorrplot(cor(trn3), lab = T, lab_size = 2)
pairs(~x6 + x10 + y, data = trn3)
lm1_df3 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x6, data = trn3)
MSE_df3_lm1 <- mean(pluck((predict(lm1_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm1
lm2_df3 <-
linear_reg() %>%
set_engine('lm') %>%
fit(y ~ x6 + x10, data = trn3)
MSE_df3_lm2 <- mean(pluck((predict(lm2_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm2
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam_df1)
coef(lasso_df2)
nB_df2 <- naiveBayes(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
nB_df2
install.packages('caret')
library(caret)
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(caret)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
?rfeIter
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = nbFnucs)
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = nbFuncs)
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = nbFuncs())
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = nbFuncs)
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = (nbFuncs))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = (functions = nbFuncs))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = (functions = 'nbFuncs'))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = ('nbFuncs'))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = c('nbFuncs'))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = 'nbFuncs')
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = 'nbFuncs')
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = nbFuncs)
rfe(X_df2, y_df2, metric = Accuracy, rfeControl = nbFuncs)
rfe(X_df2, y_df2, metric = Accuracy, rfeControl = nbFuncs)
set.seed(42)
rfe(X_df2, y_df2, metric = Accuracy, rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
set.seed(42)
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
install.packages('klar')
install.packages('klaR')
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(caret)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
library(klar)
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(caret)
library(splines)#Splines
library(pscl)# zero-inflated models
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
library(klaR)
nB_df2 <- naiveBayes(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
nB_df2
set.seed(42)
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
rfe(X_df2, y_df2, metric = 'Accuracy', rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
set.seed(42)
rfe(X_df2, y_df2, rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
set.seed(42)
rfe(X_df2, y_df2, rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
yf_df2 <- as_factor(y_df2)
set.seed(42)
rfe(X_df2, yf_df2, metric = 'Accuracy', rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
Xf_df2 <- as_list(X_df2)
Xf_df2 <- as.list(X_df2)
yf_df2 <- as_factor(y_df2)
set.seed(42)
rfe(X_df2, yf_df2, metric = 'Accuracy', rfeControl = rfeControl(functions = nbFuncs, method = 'cv', number = 8))
is.list(Xf_df2)
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
predict(lda_df2, Xtst_df2)
predict(lda_df2, tst2)
predict(lda_df2, tst2)$class
mean(predict(lda_df2, tst2)$class - tst2$y)^2
mean(as.vector(predict(lda_df2, tst2)$class) - tst2$y)^2
mean(as.numeric(predict(lda_df2, tst2)$class) - tst2$y)^2
as.numeric(predict(lda_df2, tst2)$class)
as.numeric(predict(lda_df2, tst2)$class) - 1
tst2$y
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
pred <- ifelse(predict(lda_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
as.numeric(predict(lda_df2, tst2)$class) - 1
tst2$y
#lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
#pred <- ifelse(predict(lda_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
#MSE_df2_ridge <- mean(pred - tst2$y)^2
#MSE_df2_ridge
#mean(as.numeric(predict(lda_df2, tst2)$class) - tst2$y)^2
as.numeric(predict(lda_df2, tst2)$class) - 1
tst2$y
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
MSE_df2_lda <- mean(as.numeric(predict(lda_df2, tst2)$class) - 1 - tst2$y)^2
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
lda_df2
MSE_df2_lda <- mean(as.numeric(predict(lda_df2, tst2)$class) - 1 - tst2$y)^2
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
lda_df2
MSE_df2_lda <- mean(as.numeric(predict(lda_df2, tst2)$class) - 1 - tst2$y)^2
MSE_df2_lda
X_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)[,-1]
y_df2 <- trn2$y
Xtst_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst2)[,-1]
set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda
lasso_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df2
coef(lasso_df2)
pred <- ifelse(predict(lasso_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_lasso <- mean(pred - tst2$y)^2
MSE_df2_lasso
qda_df2 <- qda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
qda_df2
MSE_df2_qda <- mean(as.numeric(predict(qda_df2, tst2)$class) - 1 - tst2$y)^2
MSE_df2_qda
var(df2_train)
for (i in 1:12) {
print(var(df2_train)[i,i])
}
var(df2_train)
for (i in 1:10) {
paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1]))
for (i in 1:10) {
paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1])
}
var(df2_train)
for (i in 1:10) {
print(paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1]))
}
for (i in 1:10) {
print(paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1], sep = ''))
}
