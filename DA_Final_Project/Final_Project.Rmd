---
title: "Final_Project"
author: "Na SeungChan"
date: "`r Sys.Date()`"
mainfont : NanumGothic
output:
  html_document:
    df_print: paged
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)#model learning
library(boot)
library(caret)
library(splines)#Splines
library(pscl)# zero-inflated models 
library(ggcorrplot)# for corrplot
library(e1071)#naive bayse
library(MASS)#lda, qda
library(tidyverse)
library(leaps)#linear model selection
library(gam)
library(glmnet)
library(klaR)
```


# Final Project

regression or classification model을 생성하여 training datasets으로 학습 후에 test datasets으로 평가할 예정입니다.

* 압축 파일에는 dataset1 ~ 20 각각 train/test set 총 40개의 csv 파일과 답을 입력하셔야 하는 test_prediction.csv 파일, 총 41개의 csv파일이 있습니다.

* 각 dataset은 train dataset (n=800)과 test dataset (n=200)으로 구성되어 있으며 train dataset은 outcome Y가 주어지지만, test dataset의 Y는 주어지지 않습니다.

* 각 train dataset의 Y 값을 확인 하시고 (binary인지, count data인지, 연속형 변수인지) 그에 맞는 모델을 사용하셔서 Y도 같은 변수로 예측하시기 바랍니다. (즉, 분류 문제의 경우 확률을 예측하는 것이 아니라, 실제 분류를 진행하셔야 합니다.)

* 여러분은 각 test dataset의 1~200번째 row의 predicted Y의 값을 "test_prediction.csv" 파일의 dataset과 번호가 맞는 column에 입력하시면 됩니다. (ex. dataset18_test의 prediction은 y18 column에 순서대로 채워주시면 됩니다.)

* test_prediction.csv 파일에는 기타 설명 등을 전혀 쓰지말고, (row의 순서대로) prediction/classification한 Y의 값만 입력해주시기 바랍니다. (ex. binary인 경우 각 cell에 0/1만 입력)


최종 제출물은

* (1) 자신이 세운 모델과 그에 대한 설명 파일 (ex. linear regression을 사용하였고, 어떤 변수를 사용하였다.. 정도로 설명하시면 됩니다.)

* (2) 실제 prediction/classification에 사용한 R code

* (3) 각 test dataset의 prediction/classification의 결과를 포함한 "test_prediction.csv" 파일 (200 row, 20 column)

세 개를 제출하여 주시면 됩니다.



# 공통 아이디어


Test Set은 사용할 수 없는 데이터로 생각하고 전혀 고려하지 않는다. Test Set의 x를 잘 관찰하면 x의 range에 대한 정보를 새로 얻을 수 있게 된다든가 하는 일이 벌어질 수도 있을 것이고, train set에서는 binary였던 x가 Test Set에서는 continuous인 악질적인 일이 있을 수도 있지만... 그런 일이 있다면 눈에 띌 것이다. 그리고, extrapolation은 모델 설정 단계부터 다항식 회귀를 적절히 penalize하는 등 방식으로 고려하는 것이 더 타당하지 Test set만 보고 해당 정보를 끌어내려 하는 것은 득보다 실이 더 크다고 판단하였다.


이런 관점에서, 다시 Train Set의 90%만 학습에 활용한다. (이후 'T set'은 학습에 활용된 90%의 Set을, 'V Set'은 나머지 10%의 Set이다.) 각 데이터셋마다 여러 모델을 fitting해 보았고, 여러 모델들 중 V Set에서의 MSE가 가장 작은 모델을 최종 모델로 선정하였다.


그리고, Binary outcome을 제출하는데 MSE로 평가된다는 것은 Accuracy로 평가한다는 것과 완벽히 동일하다. 따라서 이와 같은 문제에서 민감도와 특이도 등에 대해 고려하는 것은 우선순위에서 미룬다.


각 모델을 학습하는 과정에서 하이퍼파라미터를 결정해야 할 때는 Validation Set Approach를 최대한 피하고, k-fold approach 혹은 LOOCV를 최대한 활용하여 T set 내의 모든 정보를 활용하였다.


```{r}
final_csv <- read.csv("./dataset/test_prediction.csv")
colnames_final <- colnames(final_csv)
```



# dataset 1


```{r}
df1_train <- read.csv("./dataset/dataset1_train.csv")
df1_test <- read.csv("./dataset/dataset1_test.csv")
df1_train
summary(df1_train$y)

set.seed(42)
data_split_df1 <- initial_split(df1_train, prop = 0.9)
trn1 <- training(data_split_df1)
tst1 <- testing(data_split_df1)
```


우선 y는 연속형 반응변수이므로 이에 맞는 모델을 사용하면 된다. 그런데... 무슨 모델을 사용하는 것이 좋은지 직관적으로 전혀 알 수가 없다. 우선 lm을 시도해 본다. 


```{r}
best_select_df1 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10)
sum_lm1 <- summary(best_select_df1)

which.min(sum_lm1$cp)
which.min(sum_lm1$bic)
which.min(sum_lm1$adjr2)
```


test error 추정함수를 뭐로 쓰느냐에 따라 모델이 달라지는 것부터 불안하다. 산점도행렬을 확인하면 다음과 같다. 산점도행렬을 처음에 안 쓰는 이유는... 처음에 쓰면 직관적으로 그 어떤 것도 확인할 수 없는 그래프가 되어 버린다.


```{r}
pairs(~x2 + x5 + x7 + x9 + x10 + y, data = trn1)
pairs(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + y, data = trn1)
ggcorrplot(cor(trn1), lab = T, lab_size = 2)
```


x7에 대한 선형모형이 되어야 함은 확실해 보인다. 그리고... x9와 x10에 대해 이차함수적 관계를 y가 보이는 것 같기도 하니 이를 반영하는 모델도 fitting해 본다. 단, 다항회귀를 막 하면 오차가 발생하는 것을 생각하여 extrapolation이 발생하지 않도록 하고, x7에 대해 선형모형이 아닌 다른 모형을 적합하는 것은 오히려 variance를 발생시켜 test MSE를 늘릴 것이다. 이를 고려하는 회귀모형 적합은 GAM과 natural splines를 통해 가능하다.



```{r, eval=FALSE}
set.seed(42)
folds_df1 <- vfold_cv(trn1, v = 8)
val_err <- matrix(NA, 8, 10)
for (j in 1:8) {
  trn <- analysis(get_rsplit(folds_df1, index = j))
  tst <- assessment(get_rsplit(folds_df1, index = j))
  best_select_df1 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn, nvmax = 10)
  mtx_tst <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst)
  for (i in 1:10) {
    coefi <- coef(best_select_df1, id = i)
    pred <- mtx_tst[, names(coefi)] %*% coefi
    val_err[j, i] <-mean(tst$y - pred)^2
  }
}
val_err
mean_val_err <- apply(val_err, 2, mean)
which.min(mean_val_err)
plot(mean_val_err)
#그런데, 이 값은 매우 불안정하다. seed = 227만 봐도 알 수 있듯이... 따라서 다른 모델을 시도해 본다.
```



```{r}
coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 1)

lm1_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x7, data = trn1)

MSE_df1_lm1 <- mean(pluck((predict(lm1_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm1


coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 4)

lm2_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x2 + x5 + x7 + x9, data = trn1)

MSE_df1_lm2 <- mean(pluck((predict(lm2_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm2


coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1, nvmax = 10), id = 5)

lm3_df1 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x2 + x5 + x7 + x9 + x10, data = trn1)

MSE_df1_lm3 <- mean(pluck((predict(lm3_df1, tst1) - tst1$y)^2, 1))
MSE_df1_lm3
```



```{r}
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam1_df1)
summary(gam2_df1)

MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam2
```



lm 이외 penalized regression에 해당하는 다른 모델을 써 본다. 어떤 변수를 사용해야 할지 불분명하므로 LASSO를 사용하되, lambda의 값은 k-fold cross validation으로 결정한다.


```{r}
X_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn1)[,-1]
y_df1 <- trn1$y
Xtst_df1 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst1)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df1 <- glmnet(X_df1, y_df1, alpha = 1, lambda = bestlambda)
lasso_df1
MSE_df1_lasso <- mean(predict.glmnet(lasso_df1, Xtst_df1) - tst1$y)^2
MSE_df1_lasso
```


Basic lm에 비해 MSE의 압도적 개선이 있었다. 그러나 gam model에 비해서는 MSE가 낮은데, 이는 LASSO가 linear regression에 penalty를 더한 모델이기 때문이다. Ridge를 사용하면 값은 어떻게 되는가?


```{r}
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df1 <- glmnet(X_df1, y_df1, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean(predict(ridge_df1, Xtst_df1) - tst1$y)^2
MSE_df1_ridge
```



```{r}
MSE_df1_lm1
MSE_df1_lm2
MSE_df1_lm3
MSE_df1_gam1
MSE_df1_gam2
MSE_df1_ridge
MSE_df1_lasso
```


따라서 gam2 model을 사용한다. 이후의 연속형 변수에서도 이와 같은 논리의 흐름으로 분석이 진행되며, 분석의 기초 흐름의 설명은 다수 생략된다.


```{r}
final_csv[,2] <- predict(gam2_df1, newdata = df1_test)
```



# dataset 2


```{r}
df2_train <- read.csv("./dataset/dataset2_train.csv")
df2_test <- read.csv("./dataset/dataset2_test.csv")
df2_train
summary(df2_train)

sum(df2_train$y)/800
ggcorrplot(cor(df2_train), lab = T, lab_size = 2)

set.seed(42)
data_split_df2 <- initial_split(df2_train, prop = 0.9)
trn2 <- training(data_split_df2)
tst2 <- testing(data_split_df2)
```


우선 y가 Binary variable이므로 이에 맞춰 logistic regression, naive bayse, lda, qda 등 분류 모델에 적합한 다양한 방식을 생각할 수 있다. 한편 y = 1의 비율은 0.67875 가량이고, x4와 x5, x10이 두드러지게 y와 상관계수가 높은 것을 쉽게 볼 수 있다. x6도 애매하고... 일단 가장 간단하게 logistic regression을 선택하는데, 당연히 predictor를 싸그리 선택하면 V set MSE 퍼포먼스가 낮아질 것이고 penalized logistic regression을 선택한다.



```{r}
X_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)[,-1]
y_df2 <- trn2$y
Xtst_df2 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst2)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 1, lambda = bestlambda)
lasso_df2
coef(lasso_df2)

pred <- ifelse(predict(lasso_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_lasso <- mean(pred - tst2$y)^2
MSE_df2_lasso
```


```{r}
set.seed(42)
cv.out <- cv.glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df2 <- glmnet(X_df2, y_df2, family = 'binomial', alpha = 0, lambda = bestlambda)
ridge_df2

pred <- ifelse(predict(ridge_df2, newx = Xtst_df2, type = "response") > 0.5, 1, 0)
MSE_df2_ridge <- mean(pred - tst2$y)^2
MSE_df2_ridge
```


Naive Bayes를 실험해 본다. 


```{r}

```



```{r}
for (i in 1:10) {
  print(paste('Variance of x_', i, '=', var(df2_train)[i+1,i+1], sep = ''))
}

```



```{r}
lda_df2 <- lda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
lda_df2
MSE_df2_lda <- mean(as.numeric(predict(lda_df2, tst2)$class) - 1 - tst2$y)^2
MSE_df2_lda
```


```{r}
qda_df2 <- qda(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn2)
qda_df2
MSE_df2_qda <- mean(as.numeric(predict(qda_df2, tst2)$class) - 1 - tst2$y)^2
MSE_df2_qda
```









# dataset 3


```{r}
df3_train <- read.csv("./dataset/dataset3_train.csv")
df3_test <- read.csv("./dataset/dataset3_test.csv")
df3_train
summary(df3_train)
pairs(~., data = df3_train)
hist(df3_train$y)

set.seed(42)
data_split_df3 <- initial_split(df3_train, prop = 0.9)
trn3 <- training(data_split_df3)
tst3 <- testing(data_split_df3)
```


우선 y는 연속형 반응변수이므로 이에 맞는 모델을 사용하면 된다. 그런데... 무슨 모델을 사용하는 것이 좋은지 직관적으로 전혀 알 수가 없다. 그런데 y의 자료구조가 좀 이상하다...?


```{r}
df3_train %>% filter(y > 10)
trn3_drop <- trn3 %>% filter(y < 7.4)
```

이와 같이, y>10인 자료가 5개뿐일 정도로 y의 분포가 극단적인 양의 왜도를 가진다. 해당 분포에서 y의 왜도가 얼마나 체계적인지 예측할 수 있는 변수를 찾아낼 수 있다면 best지만, 고작 5개 변수로 그런 게 되기 어렵다... 

어디부터 어디까지가 outlier인지는 분석가의 주관에 달린 영역이고, 해당 분석에서는 편의를 위해 임의로 y > 7.4 이상의 값을 outlier로 보았다. 사실 y=10 등 값이 문턱으로 더 합리적이지만, y = 7.4로 두면 집합 분할 개수가 배수로 딱 맞는 게 이유일 뿐이다. 어차피 마지막에 MSE로 outlier를 뺀 분석과 빼지 않은 분석이 모두 시행되고, outlier를 빼고 fitting한 model과 outlier를 넣고 fitting한 model 중 MSE가 더 작은 모델을 선택할 것이니 아무 문제 없다.


우선 lm을 시도해 본다. model selection 이후 lm을 fitting해 본다.


```{r}
best_select_df3 <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10)
sum_lm3 <- summary(best_select_df3)

which.min(sum_lm3$cp)
which.min(sum_lm3$bic)
which.min(sum_lm3$adjr2)

coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 1)

coef(regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3, nvmax = 10), id = 2)
```


```{r}
ggcorrplot(cor(trn3), lab = T, lab_size = 2)
pairs(~x6 + x10 + y, data = trn3)
```


interaction으로 인해 회귀계수 문제가 생긴 부분은 크지 않아 보인다. 


```{r}


```



```{r}
lm1_df3 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6, data = trn3)

MSE_df3_lm1 <- mean(pluck((predict(lm1_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm1


lm2_df3 <-
  linear_reg() %>%
  set_engine('lm') %>%
  fit(y ~ x6 + x10, data = trn3)

MSE_df3_lm2 <- mean(pluck((predict(lm2_df3, tst3) - tst3$y)^2, 1))
MSE_df3_lm2
```


lm 이외 다른 모델을 써 본다. gam을 우선 사용하며, 


```{r}
gam1_df1 <- gam(y ~ x2 + x5 + x7 + ns(x9), data = trn1)
gam2_df1 <- gam(y ~ x2 + x7 + ns(x9), data = trn1)
summary(gam1_df1)
summary(gam2_df1)

MSE_df1_gam1 <- mean(pluck((predict(gam1_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam1
MSE_df1_gam2 <- mean(pluck((predict(gam2_df1, newdata = tst1) - tst1$y)^2, 1))
MSE_df1_gam2
```





LASSO를 사용하되, lambda의 값은 k-fold cross validation으로 결정한다.


```{r}
X_df3 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = trn3)[,-1]
y_df3 <- trn3$y
Xtst_df3 <- model.matrix(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = tst3)[,-1]

set.seed(42)
cv.out <- cv.glmnet(X_df3, y_df3, alpha = 1, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

lasso_df3 <- glmnet(X_df3, y_df3, alpha = 1, lambda = bestlambda)
lasso_df3
MSE_df3_lasso <- mean(predict.glmnet(lasso_df3, Xtst_df3) - tst3$y)^2
MSE_df3_lasso

set.seed(42)
cv.out <- cv.glmnet(X_df3, y_df3, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df3 <- glmnet(X_df3, y_df3, alpha = 0, lambda = bestlambda)
ridge_df3
MSE_df3_ridge <- mean(predict.glmnet(ridge_df3, Xtst_df3) - tst3$y)^2
MSE_df3_ridge
```


Basic lm에 비해 MSE의 압도적 개선이 있었다. Ridge를 사용하면 값은 어떻게 되는가?


```{r}
set.seed(42)
cv.out <- cv.glmnet(X_df1, y_df1, alpha = 0, nfolds = 8)
bestlambda <- cv.out$lambda.min
bestlambda

ridge_df1 <- glmnet(X_df1, y_df1, alpha = 0, lambda = bestlambda)
ridge_df1
MSE_df1_ridge <- mean(predict.glmnet(ridge_df1, Xtst_df1) - tst1$y)^2
MSE_df1_ridge
```



```{r}
MSE_df3_lm1
MSE_df3_lm2
MSE_df3_ridge
MSE_df3_lasso
```

와 같으므로 ridge model을 사용한다. dataset 1에 비해 linear model의 performance가 더 나빠진 이유는? 특정 변수의 영향력이 독보적으로 큰 상황이라 그것만 넣어도 중간은 가는 상황이 아니라서...


```{r}
testX <- model.matrix(~x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df1_test)[,-1]

final_csv[,2] <- predict(lasso_df1, newx = testX)[,1]
```



# dataset 4




```{r}
df4_train <- read.csv("./dataset/dataset4_train.csv")
df4_test <- read.csv("./dataset/dataset4_train.csv")
df4_train
summary(df4_train$y)
```










# dataset 5


```{r}
df5_train <- read.csv("./dataset/dataset5_train.csv")
df5_test <- read.csv("./dataset/dataset5_train.csv")
df5_train
summary(df5_train$y)
```








# dataset 6


```{r}
df6_train <- read.csv("./dataset/dataset6_train.csv")
df6_test <- read.csv("./dataset/dataset6_train.csv")
df6_train
summary(df6_train$y)
```







# dataset 7


```{r}
df7_train <- read.csv("./dataset/dataset7_train.csv")
df7_test <- read.csv("./dataset/dataset7_test.csv")
df7_train
summary(df7_train$y)
```


response variable y는 count variable이다. poisson regression, negative binomial regression, zero-inflated poisson regression, zero-inflated negative binomial regression을 고려하자.


```{r}
mean(df7_train$y)
var(df7_train$y)
```


overdispersed. poisson regression, zero-inflated poisson regression은 적절하지 않다.



```{r, eval = FALSE}
poisson_df7_full <- glm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df7_train, family = 'poisson')
summary(poisson_df7_full)
cv.err <- cv.glm(df7_train[,-1], poisson_df7_full)
cv.err$delta
```


full model에서 몇몇 계수들은 영향력이 크고, 몇몇 계수들은 영향력이 거의 없는 것으로 나오므로 이를 제외하고 reduced model을 fitting해 본다.



```{r, eval=FALSE}
poisson_df7_reduced <- glm(y ~ x2 + x7 + x8 + x10, data = df7_train, family = 'poisson')
summary(poisson_df7_reduced)
cv.err <- cv.glm(df7_train[,-1], poisson_df7_reduced)
cv.err$delta
```


cv.error의 0.5 가량 감소가 있었다. 이로부터 poisson regression을 사용하는 경우, reduced model을 사용하는 것이 적절함을 안다. 



```{r, eval=FALSE}
negbin_df7_full <- glm.nb(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10, data = df7_train)
summary(negbin_df7_full)
cv.err <- cv.glm(df7_train[,-1], negbin_df7_full)
cv.err$delta
```


full model에서 몇몇 계수들은 영향력이 크고, 몇몇 계수들은 영향력이 거의 없는 것으로 나오므로 이를 제외하고 reduced model을 fitting해 본다.


```{r,eval=FALSE}
negbin_df7_reduced <- glm.nb(y ~ x2 + x7 + x8 + x10, data = df7_train)
summary(negbin_df7_reduced)
cv.err <- cv.glm(df7_train[,-1], negbin_df7_reduced)
cv.err$delta
```

cv.error의 0.18 가량 감소가 있었다. 이로부터 negative binomial regression을 사용하는 경우, reduced model을 사용하는 것이 적절함을 안다. 


```{r,eval=FALSE}
zero_infl_negbin <- zeroinfl(y ~ x2 + x7 + x8 + x10|x2 + x7 + x8 + x10, data = df7_train, dist = 'negbin')
summary(zero_infl_negbin)
```


```{r,eval=FALSE}
predict(negbin_df7_reduced, newdata = df7_test)

plogis()
```









































































































































































































































































































































































































































































































































































































































































































```{r}
final_csv <- set_names(final_csv, colnames_final)

```

