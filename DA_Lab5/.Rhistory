knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
# install.packages("textdata")
library(textdata)
afinn <- get_sentiments("afinn")
# install.packages("textdata")
library(tidytext)
library(textdata)
afinn <- get_sentiments("afinn")
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
library(aRxiv)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE)
library(aRxiv)
DataSciencePapers_arxiv <- arxiv_search(
query = '"Data Science"',
limit = 20000,
batchsize = 100
)
library(mdsr)
data(DataSciencePapers)
DataSciencePapers
library(dplyr)
glimpse(DataSciencePapers)
library(lubridate)
DataSciencePapers <- DataSciencePapers %>%
mutate(
submitted = lubridate::ymd_hms(submitted),
updated = lubridate::ymd_hms(updated)
)
glimpse(DataSciencePapers)
mosaic::tally(~ year(submitted), data = DataSciencePapers)
# Or, you can simply use :
table(year(DataSciencePapers$submitted))
DataSciencePapers %>%
filter(id == "1809.02408v2") %>%
glimpse()
DataSciencePapers %>%
group_by(primary_category) %>%
count() %>%
head()
library(stringr)
DataSciencePapers <- DataSciencePapers %>%
mutate(
field = str_extract(primary_category, "^[a-z,-]+"),
)
# DataSciencePapers <- DataSciencePapers %>%
#   mutate(
#     field = str_extract(primary_category, "^[A-Z,-]+"),
#   )
mosaic::tally(x = ~field, margins = TRUE, data = DataSciencePapers) %>%
sort()
#install.packages("tidytext")
library(tidytext)
DataSciencePapers %>%
unnest_tokens(word, abstract) %>%
count(id, word, sort = TRUE)
#install.packages("stopwords") #You need to install the package "stopwords"
library(stopwords)
arxiv_words <- DataSciencePapers %>%
unnest_tokens(word, abstract) %>%
anti_join(get_stopwords(), by = "word")
arxiv_words %>%
count(id, word, sort = TRUE)
arxiv_abstracts <- arxiv_words %>%
group_by(id) %>%
summarize(abstract_clean = paste(word, collapse = " "))
arxiv_papers <- DataSciencePapers %>%
left_join(arxiv_abstracts, by = "id")
single_paper <- arxiv_papers %>%
filter(id == "1809.02408v2")
single_paper %>%
pull(abstract) %>%
strwrap() %>%
head()
single_paper %>%
pull(abstract_clean) %>%
strwrap() %>%
head(4)
#install.packages(c("wordcloud","RColorBrewer"))
library(wordcloud);library(RColorBrewer)
set.seed(1966)
arxiv_papers %>%
pull(abstract_clean) %>%
wordcloud(
max.words = 40,
scale = c(8, 1),
colors = topo.colors(n = 30),
random.color = TRUE
)
library(tidytext)
library(textdata)
afinn <- get_sentiments("afinn")
afinn %>%
slice_sample(n = 15) %>%
arrange(desc(value))
arxiv_words %>%
inner_join(afinn, by = "word") %>%
select(word, id, value)
arxiv_sentiments <- arxiv_words %>%
left_join(afinn, by = "word") %>%
group_by(id) %>%
summarize(
num_words = n(),
sentiment = sum(value, na.rm = TRUE),
.groups = "drop"
) %>%
mutate(sentiment_per_word = sentiment / num_words) %>%
arrange(desc(sentiment))
arxiv_papers <- arxiv_papers %>%
left_join(arxiv_sentiments, by = "id")
arxiv_papers %>%
skim(sentiment, sentiment_per_word)
most_positive <- arxiv_papers %>%
filter(sentiment_per_word == max(sentiment_per_word)) %>%
pull(abstract)
strwrap(most_positive)
ggplot(
arxiv_papers,
aes(
x = submitted, y = sentiment_per_word,
color = field == "cs"
)
) +
geom_smooth(se = TRUE) +
scale_color_brewer("Computer Science?", palette = "Set2") +
labs(x = "Date submitted", y = "Sentiment score per word")
arxiv_words %>%
count(word) %>%
arrange(desc(n)) %>%
head()
tidy_DTM <- arxiv_words %>%
count(id, word) %>%
bind_tf_idf(word, id, n)
tidy_DTM %>%
arrange(desc(tf)) %>%
head()
tidy_DTM %>%
arrange(desc(idf), desc(n)) %>%
head()
arxiv_papers %>%
pull(abstract) %>%
str_subset("wildfire") %>%
strwrap() %>%
head()
tidy_DTM %>%
filter(word == "implications")
tidy_DTM %>%
filter(id == "1809.02408v2") %>%
arrange(desc(tf_idf)) %>%
head()
tidy_DTM %>%
filter(word == "covid") %>%
arrange(desc(tf_idf)) %>%
head() %>%
left_join(select(arxiv_papers, id, abstract), by = "id")
tidy_DTM %>%
arrange(desc(tf_idf)) %>%
head() %>%
left_join(select(arxiv_papers, id, abstract), by = "id")
tm_DTM <- arxiv_words %>%
count(id, word) %>%
cast_dtm(id, word, n, weighting = tm::weightTfIdf)
tm_DTM
library(tm)
findFreqTerms(tm_DTM, lowfreq = 7)
library(purrr)
tm_DTM %>%
as.matrix() %>%
as_tibble() %>%
map_dbl(sum) %>%
sort(decreasing = TRUE) %>%
head()
library(rvest)
url <- "http://en.wikipedia.org/wiki/List_of_songs_recorded_by_the_Beatles"
tables <- url %>%
read_html() %>%
html_nodes("table")
Beatles_songs <- tables %>%
purrr::pluck(3) %>%
html_table(fill = TRUE) %>%
janitor::clean_names() %>%
select(song, lead_vocal_s_d)
glimpse(Beatles_songs)
Beatles_songs <- Beatles_songs %>%
mutate(song = str_remove_all(song, pattern = '\\"')) %>%
rename(vocals = lead_vocal_s_d)
Beatles_songs %>%
group_by(vocals) %>%
count() %>%
arrange(desc(n))
Beatles_songs %>%
pull(vocals) %>%
str_subset("McCartney") %>%
length()
Beatles_songs %>%
pull(vocals) %>%
str_subset("Lennon") %>%
length()
Beatles_songs %>%
pull(vocals) %>%
str_subset("(McCartney|Lennon)") %>%
length()
pj_regexp <- "(McCartney|Lennon).*(McCartney|Lennon)"
Beatles_songs %>%
pull(vocals) %>%
str_subset(pj_regexp) %>%
length()
Beatles_songs %>%
filter(str_detect(vocals, pj_regexp)) %>%
select(song, vocals) %>%
head()
Beatles_songs %>%
unnest_tokens(word, song) %>%
anti_join(get_stopwords(), by = "word") %>%
count(word, sort = TRUE) %>%
arrange(desc(n)) %>%
head()
library(tidyverse)
library(mdsr)
library(nycflights13)
SF <- flights %>%
filter(dest == "SFO", !is.na(arr_delay))
three_flights <- SF %>%
slice_sample(n = 3, replace = FALSE) %>%
select(year, month, day, dep_time)
three_flights
three_flights %>% slice_sample(n = 3, replace = TRUE)
three_flights %>% slice_sample(n = 3, replace = TRUE)
n <- 200
orig_sample <- SF %>%
slice_sample(n = n, replace = FALSE)
orig_sample %>%
slice_sample(n = n, replace = TRUE) %>%
summarize(mean_arr_delay = mean(arr_delay))
num_trials <- 200
sf_200_bs <- 1:num_trials %>%
map_dfr(
~orig_sample %>%
slice_sample(n = n, replace = TRUE) %>%
summarize(mean_arr_delay = mean(arr_delay))
) %>%
mutate(n = n)
sf_200_bs %>%
skim(mean_arr_delay)
sf_200_pop <- 1:num_trials %>%
map_dfr(
~SF %>%
slice_sample(n = n, replace = TRUE) %>%
summarize(mean_arr_delay = mean(arr_delay))
) %>%
mutate(n = n)
sf_200_pop %>%
skim(mean_arr_delay)
strings <- c(
"This string has no hashtags",
"#hashtag city!",
"This string has a #hashtag",
"This string has #two #hashtags"
)
text_lines <- tibble(
lines = c("This is the first line.",
"This line is hyphen- ",
"ated. It's very diff-",
"icult to use at present.")
)
